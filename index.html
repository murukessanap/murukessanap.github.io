<!DOCTYPE html>
<html lang="en">
    <style>
  .box {
    width: 300px;
    height: 50px;
    border: 2px solid #333;
    background-color: #f0f0f0;
    text-align: center;
    line-height: 50px; /* Align text vertically */
    margin: 0px;
  }
</style>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MathJax Example</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    Hello World

<h1>Today topic is Recommender Systems</h1>

Lets say a company like Netflix, Amazon, Disney Hotstar need to recommend the correct user profile the matching movie, so that there is a high chance that many users
will see the recommended movie thereby increasing profit
for the companies in many ways.
</br>
</br>
</br>
Lets see what are all the data available for the recommender system to learn from. There are user ratings movie, user profile information and movie information.
</br>
User rating movie is taken from the movie reviews given by users.
</br>
User profile information is all the information they ask you and you give them when you create an account and after which anything you do in the website, can also 
be tracked to have more info about the user, like watching a trailer, searching for a genre, even avoiding some content, etc.
</br>
Movie information is like the cast in the movie, genre, director, producing company, animated or not, country of origin, etc.
</br>
</br>
</br>
Lets assume all these information as matrices like user-movie rating as \(M_{um}\), user-profile as \(M_{up}\) and movie-profile as \(M_{mp}\)
</br>
    Now the target is to find missing ratings that are not given by the users for movies in \(M_{um}\)
    </br>
    Lets say the dimensions of the matrices \(M_{um}\) is \(_{uxm}\), \(M_{up}\) is \(_{f1xu}\) and \(M_{mp}\) is \(_{mxf2}\)
        </br>
        </br>
        Lets take weight matrices \(M_u\) and \(M_m\) that learns user features and movie features.
        </br>
        Dimensions of \(M_m\) and \(M_u\) are \(_{f2xu}\) and \(_{f1xm}\) respectively.
</br>
</br>
</br>
    The optimization objective is to learn \(M_u\) and \(M_m\) in the following setting.
    </br></br>
        \(M_{up}^T\) . \(M_u\) = \(M_{um}\)  and (1)
    </br>
        \(M_{mp}\) . \(M_m\) = \(M_{um}^T\)  and (2)
    </br></br>
There are various ways to solve this optimization objective.
</br>
One of them is through Gradient Descent Algorithm.
</br>
Randomized initial weights of the matrices \(M_u\) and \(M_m\) are updated via gradient descent by minimizing the following loss function.
</br></br>
    \(E(M_{um}^p - M_{um})^2\)  condition i-th element in \(M_{um}\) is not empty and where \(M_{um}^p\) is the predicted matrix and it is a sum of squarred errors of elementwise difference
    in the predicted and real user movie ratings matrix. 

</br>
</br>
</br>
    They call this colloborative filtering since there is a colloboration of all users' ratings, user profile information and movie information for
    getting the user the right recommendation. Also the practical implementation of gradient descent will be from solving (1) to (2) to (1) to (2) to (1) .....
    </br></br>
    Remember anything from social media, youtube, even ads on all of them are a form of recommendation.
</br>
    Have a wonderful recommendation !!!

    I owe this article in open forum to Andrew Ng, co-founder of Coursera. Thank you Andrew.
</br>
</br>
</br>

<h1>Today's topic is Information Theory</h1>
    The information theory was proposed by Shannon with his famous coining term entropy to quantify information as suggested by Von Neumann. But lets take a dive into what could be 
    its use as an overall information theory for the human race. With such a core thought in mind, let me elucidate the information theory. Lets visit a different science actually physics by 
    Boltzman when he had to coin the entropy for physical systems such as a group of atoms juggling in a container. The question was to find the initial set of possible states of the atoms and their
    relative positions perhaps other properties may be like motion therefore heat and so on. So if you have two states for every atom as position and velocity. Then for different possible combinations of 
    the initial states of the atoms in different positions and velocities, it could end up in exponential possible states in the flow of time. The exponentiality increases over time and again.
    Lets consider an example, assume there are 10 different spacial points where atoms can possibly stay and 8 different directions in which they can have their velocities. Let there be some 7 different 
    possible velocities. This may seem absurd like the space, direction, speed in reality may seem to be in real number space. But in atomic quantum states it may be different. It could be quantized.

</br>
</br>
    So if you have an atom, it can have possible initial 10*8*7 states. Lets say there are two atoms then there are 10*8*7 possible initial states for each of them that start to interact with each other over time.
    So when there are more than one atom, you need to formulate some interacting principles between them and define the possibility of the states that may end up over time. The truth being if you have more
    atoms, with more freedom of space, direction and speed, whatever the interacting principles being the same for all atoms may end up in exponentially more and more possible states over time. With the caveat,
    the states may even repeat over time given the number of possible states of systems reaching a limit in the system with rules and the atoms. If you have to find out the number of intial states or at least the 
    repeating states of any n-body atomic system, given the possibilities of the states they are in over time, You need to do logarithm  of all the possible states they end up over time. Let me elucidate with a simple 
    digital number system. Lets say in binary number system there are two possibilities for every digit 0 or 1. But if you have more than 1 digit, there is an exponentially increasing possible representative numbers or
    possible states. Like 1 digit has 2 states(0|1), 2 digits has 4 states(00|01|10|11), 3 digits has 8 states(000|001|010|011|100|101|110|111) and 4 digits has 16 states. So it is exponentially increasing
    over the increase of no of digits \(2^n\). Then think of the single bit having 10*8*7 possible states, then it could well possibly be \((10*8*7)^n\) states to end up in n amount of time.
    So to find the amount of information in so many possibilities one has to do logarithm, like if you have 8 numbers in a binary system then \(log_28=3\) there are 3 digital bits of information. If you have 
    \((10*8*7)^5\) atomic states then \(log_{10*8*7}{10*8*7}^5=5\) resulting in 5 atoms being present in the inital state.

</br>
</br>
    The mathematical beauty of Shannon's theory lies in quantifying information over the probability of the loss of information. Lets say the inital state of atoms are not deterministic but
    probabilistic like there is a probability of 0.3 for say 1000 states, then 0.4 for say 2000 states and 0.3 for say 3000 states. Then the information in those possible states is 
    \( 0.3*(1000/6000) + 0.4*(2000/6000) + 0.3*(3000/6000) = 2000/6000 = 0.333... \). But how can you get that information by seeing the system evolve over time over loss of information like in transmitting information over 
    any communication systems with a lot of probability of interference or error resulting in loss of information? So Shannon invented the formula 
</br>
</br>
    \( Entropy = -\sum_i p_i * log (p_i) \)
</br>
</br>
    It is just the weighted sum of logarithm of the possible informative states or expectation of information in probability. But it is negative because entropy tells us about disorder in the information and not the order.
    Therefore the less the entropy more the information. Let me show a picture of probability of a single event with two possiblities 0 being the event did not happen and 1 being the event happened. Like 
    it rains or not, the girl loves or not, will I pass or not, etc. mapped with the entropy formula. Here I have done a logarithm of base 10, whereas normally base 2 is used as a unit for entropy.
    You can see the entropy is high in the unsure probabilty. If a clear yes/no then there is more information than entropy.
</br>
    <img src="entropy.png" title="probabilty of a binary event over its entropy contribution p*log(p) in the sum" width="500" height="300">
</br>
    Now lets see how this information theory could well be utilized to find out which religion is right and which one is wrong, which one was the initial condition? Like did it all start with Adam and Eve.
    Out of hunger did they start to reproduce naturally disobeying command of God or by the information from Satan. Did all humans who were from the same family of Adam and Eve reproduced among each other? Did God 
    reproduced asexually to give birth to Satan? Is God a male, female, LQBTQ? Or everything is from nothing and there is nothing beyong that nothing? Did beauty and attention give rise to Satan or finding
    God in beauty made him send Satan. For Christ's sake how many chicken egg problems are there in the world I get confused a lot. At least still I saw Aishwarya Rai, Hritik Roshan, Steve Reeves, 
    John Abhraham, Marlyn Manroe, Arnold, Stallone, Rashmika Mandanna, so on, I defined beauty in my own way. Having not seen much of any mirror myself. At least there were no filters except makeups and 
    possibly steroids in those days. But have the humans having seen the celebrities' pictures in their primates' era, would have commited self racial suicide trying to outrun the imaginary beauty on their own.
    Have the humans started face shaming each other only to find may be in a pond their own self and started considering everyone ugly. Or after seeing the pond self image they started face shaming. Again 
    sham-beauty or beauty-gets-shammed chicken egg problem. I am confused therefore I turned non-veg and eat both chicken and egg.
</br>
</br>
    Lets simplify to love and hate. Most of the people find God in love and Satan in hate. Equally people find God in hate and Satan in love. Just they define and redefine on their enemies. If 
    we have four bits of information for love/hate like [Self|Family|Others|Enemy]. There are possibily 16 different states, like you love yourself and manipulate everyone else becoming narcissistic. You 
    love yourself and family but not others and therefore all turn enemies. You love yourself, family, others but not enemy. You love all even enemy and they make sure you get crucified. You love yourself, but
    not your family, but you love others and even enemy. All possible states of love/hate over self,family,others,enemy exist in over the world in people and it may change course over time as well. 
    If you want to find the religious secrets like God and Satan, love and hate by being inside the system of such love and hate, God and Satan, giving rise to the multiversal interpretation of the
    Quantum physics, then you need to fight the entire world to get it. Because you or your n-body team will always be in a state collapse giving rise to enemies in every possible timeline. 
</br>
</br>
    If you want to simplify, lets say you have to face others for survival and you may never know exactly what your work is for love or hate, if it can be used for love or hate over time. If you see God
    in love but don't want to get cruicified then you need to do random lovable things of free will to your own self and the family you love. It can be a great or a small thing, like financial support or 
    cooking, anything which you can do on your own, out of love. This may keep the balance of love in the enemy n-body and keep enemies from hurting each other. If you find God in hate, make sure you hate 
    yourself also and ready to do the things you do to others, enemy or even family to your own self. If you have to keep a balance you have to give your equal rights to the opposite gender especially wife.
    Equal rights in everything especially in sex too. If you can have multiple partners then your wife can too. If not don't talk about balance. Hate is not bad unless you are ready to put a bullet
    through having lost Berlin. Or having done all nonsensical hatred to all in hidden ways for food, survival, and sex, you get to know the same things gets done to you in your death bed. You say a 
    good bye to again obey the command of God by not eating the apple. Also remember the more you make peace with others, and do not manipulate others to get to your enemy, the less entropy you generate yourself,
    thereby may be peace of mind.

</br>
</br>
    Oh Christ, save me from all chickens and eggs.


    <h1>
        Euler resonates in P!=NP
    </h1>

    Let me elucidate the story of Euler discovering the mathematical constant e. Those who have done logarithm know natural logarithm is e. Let's see how Euler may have discovered the
    famous mathematical constant that redefined maths, therefore, physics and all sciences. First, the problem was studied extensively by the mathematician Bernoulli family, who found the
    exceptional mathematical talent of Euler to encourage him to pursue maths rather than theology, even convincing his parents. Bernoulli family studied the compound interest in the banking system
    in which your interest is calculated also with the capital for further interest. Let's say you deposited money 1000$ with a simple interest of 1% for every month then you can calculate
    your simple interest from the simple formula PNR where P is the principal amount, N is the unit of time for the rate of interest R in fractional percentage. So here, simple interest 
    is if for a year 12% rate of interest or twelve months of 1% interest is 1000*12/100 = 120$. This is simple, but banking wanting to compete with each other being the strongest 
    money influence in the state introduced compound interest. It is like every month, interest calculated will accrue with the principal to again form new interest like the first month's 
    interest is 1000*1/100 = 10$, but the next month's principal for calculation is 1010$, so 1010*1/100 = 10.1$. The next month's principal for calculation is 1020.1 $ so it keeps on
    accruing every month by the formula 

    </br>
    <div class="box">
     \( A = P \times (1+r/n)^{n \times t} \)
    </div>
    </br>

    Where A is the future value, P is the Principal, r is the rate of interest annually, n is the no of times interest is compounded per year, and t is the no of years of
    investment. In our case A = 1000 *(1+0.12/1)^1 = 1120$. But if you calculate for n being more than every year interest calculated for, say, 3 times and for 4 years, 
    then A = 1000*(1+0.12/3)^(3*4) = 1000*(1.04)^12 = 1000*1.6010322185676 = 1601.0322185676$. So if you see a simple 12 percent interest annually, interest calculated
    three times a year, for four years, one needs to calculate to so many decimals to clients for being precise in the business of dealing money. And the decimals become more 
    valuable with the principal being large for it will make value not be rounded. It makes one think that big investors need to be impressed with precise calculations of 
    interest. Also, without calculators, the precision being manual calculation became a great source of error in loss of business due to customer disapproval. So Euler 
    set on a mission to solve this problem. He realized one needs to find the functional solution given the highest limit to the problem, which is 100% interest annually, 
    and every year number of interest calculations reaches infinity. So the function for interest fraction to be multiplied with principal becomes 

    </br>
    <div class="box">
    \( y = Lt_{n->inf}(1+1/n)^n \) 
    </div>
    </br>

    If you substitute the limit n=inf, y=1 it won't be the case for n will approach infinity but won't be exact infinity in any real calculation to banking
    business. Also, there is a paradox of division and its inverse in the very function. For if you substitute x = 1/n, we get

    </br>
    <div class="box">
    \( y = Lt_{x->0}(1+x)^{1/x} \) 
    </div></br>

    The inverse of any number being existent like zero, doesn't have an inverse, only infinity, and other inverses like cube roots are difficult to calculate. So to make the 
    function complete in divisional inverse, one needs to take into account n turning 1/n and vice versa. So, the function should be studied for 

    </br>
    <div class="box">
    \( y = Lt_{x | 1/x -> inf | 0}(1+1/x)^x \)  
    </div></br>

    Leading to four possible solutions

    </br>
    <div class="box">
    \( y = Lt_{x -> inf}(1+1/x)^x \)  
    </div></br>

    </br>
    <div class="box">
    \( y = Lt_{x -> 0}(1+1/x)^x \)  
    </div></br>

    </br>
    <div class="box">
    \( y = Lt_{1/x -> inf}(1+1/x)^x \)  
    </div></br>

    </br>
    <div class="box">
    \( y = Lt_{1/x -> 0}(1+1/x)^x \)  
    </div></br>
    

    This leads to indeterminate solutions so to solve one needs to use the  logarithm

    </br><div class="box">
    \( log(y) = Lt_{x->inf}[x log(1+1/x)] \)</div> </br>
    <div class="box">
    \(       = Lt_{x->inf}[log(1+1/x)/(1/x)] \) </div> </br>
    let z=1/x
    <div class="box">
    \(       = Lt_{z->0}[log(1+z)/z] \) </div> </br>
    <div class="box">
    \(       = Lt_{z->0}(1/(1+z)) \) </div></br>
    <div class="box">
    \(       = 1 \) </div></br>
    Apply L.Hospital's Rule  
    </br>

    </br><div class="box">
    \( log(y) = Lt_{x->0}[x log(1+1/x)] \)</div> </br>
    <div class="box">
    \(       = Lt_{x->0}[log(1+1/x)/(1/x)] \) </div> </br>
    let z = 1/x
    <div class="box">
    \(       = Lt_{z->inf}[log(1+z)/z] \) </div> </br>
    <div class="box">
    \(       = Lt_{z->inf}(1/(1+z)) \) </div></br>
    <div class="box">
    \(       = 0 \) </div></br>
    Apply L.Hospital's Rule. Again trivial for closed-form solutions. This makes one think log reduces an entire function to zero, which is the equivalent that any base to this log function turns zero. To make any number zero, it should be
    powered with -inf which is y turns \( a^{-inf} \) which is trivial for closed-form convergence

    </br><div class="box">
    \( log(y) = Lt_{1/x->inf}[x log(1+1/x)] \)</div> </br>
    <div class="box">
    \(       = Lt_{x->0}[log(1+1/x)/(1/x)] \) </div> </br>
    let z=1/x
    <div class="box">
    \(       = Lt_{z->inf}[log(1+z)/z] \) </div> </br>
    <div class="box">
    \(       = Lt_{z->inf}(1/(1+z)) \) </div></br>
    <div class="box">
    \(       = 0 \) </div></br>
    Apply L.Hospital's Rule. Again trivial for closed-form solutions

    </br><div class="box">
    \( log(y) = Lt_{1/x->0}[x log(1+1/x)] \)</div> </br>
    <div class="box">
    \(       = Lt_{x->inf}[log(1+1/x)/(1/x)] \) </div> </br>
    let z=1/x
    <div class="box">
    \(       = Lt_{z->0}[log(1+z)/z] \) </div> </br>
    <div class="box">
    \(       = Lt_{z->0}(1/(1+z)) \) </div></br>
    <div class="box">
    \(       = 1 \) </div></br>
    Apply L.Hospital's Rule 

    The first option and fourth option are where y turns one with a log for any base. Also, the first and fourth options resemble the same function if you invert their limits. And second and third options resemble
    the same function you invert their limits. This gives the logarithm base independent nature but makes y zero even when applied to any base number
    which makes log(y)= 1 always. 

    </br>
    <div class="box">
    \( log [Lt_{x->inf}[(1+1/x)^x]] = 1 \) </div> </br>

    </br>
    <div class="box">
    \( Lt_{x->inf}[(1+1/x)^x] = e \) </div> </br>

    </br>
    <div class="box">
    \( log(e) = 1 \) </div></br>

    This defines a new constant in the logarithm. This constant unifies all bases in the logarithm, and if you see it also defines the inverse in the logarithmic base if you
    multiply \( log_e(b) \) with \( log_b(x) \), it gives \( log_e(x) \), which is independent of its base due to \( log(e)=1 \) being the unity in the log base group algebra.
    Logarithmic inverse turned out to be an infinite series if you keep on multiplying term after term approaching infinity and, therefore, more and more decimal points.
    </br>
    
    Let us introduce a variable for e and a new function. 

    </br>
    <div class="box">
    \( y = e^x \) </div> </br>

    </br>
    <div class="box">
    \( log(y) = x log(e) \) </div> </br>

    </br>
    <div class="box">
    \( log(y) = x \) </div></br>

    Differentiating the equation, we get

    </br>
    <div class="box">
    \( (1/y)y^{'} = 1 \) </div> </br>

    </br>
    <div class="box">
    \( y^{'} = y \) </div></br>

    </br>
    <div class="box">
    \( y^{''} = y^{'} \) </div></br>

    </br>
    <div class="box">
    \( y^{'''} = y^{''} \) </div> </br>

    It is a unity in differentiation, a least soluble analytical solution of exponential functions that are hard to solve. Even if you have a one more power variable like
    the same independent variable x for the log like the equation 
    \( y = x^x \) then 
    \( log(y) = xlog_e(x) \), it leads to x not disappearing in the differential equation reducing the independent
    variable to a function y. It leads to dependence between two variables getting into a single non-linear differential equation and more dependency in further 
    differentiation of the same. This makes one think that e is the most elegant solution for the non-linear differential equation. But e itself requires the Taylor series
    for approximation of at least ten terms to do calculations of more precision. If you think of any more analytical equations like non-linear differential equations like
    \( y=a^x \) itself requires log and e for most precision. It defines the precision limit in solving analytical complete closed-form solutions to switch from 
    numerical methods to approximation to neural networks with universal approximation. 

    </br>
    Let us link it with a known NP-complete problem, Travelling Salesman Problem{TSP}, in which a graph is defined with edges and weights. Nodes are cities connected in the graph and
    the edges are adjacent cities with weights as travel distance between them. The problem is to find a route by starting from a single city and covering all given cities to 
    return back to the start city by covering the minimum distance. They say this problem is NP-complete due to no known existing polynomial algorithm with a Deterministic Turing
    machine has been found yet, and founding even one such algorithm will lead to a polynomial solution to all NP-complete problems. Therefore P=NP. NP is a non deterministically 
    polynomial; if you have a choice() function that does choose on its own without a calculation, then this problem is polynomial. If P=NP, most of your daily need 
    computing power can well be taken from the mobile itself and not require heavy processing servers GPUs cloud which can be minimal for networking. But P!=NP approximation numerical
    solutions are a must for more complex functions.
    </br>
    If you take TSP, it is defined in memory by an adjacency matrix of nodes in columns and rows as a square matrix of nxn where n is the no of nodes in the graph with all the matrix
    values as the edges in the graph or the distance between the nodes, Also note that an NP-Complete problem is polynomially verifiable in computing time. Like if you define the minimum distance 
    between any two nodes as 1 then the final solution must be equal to the of cities to be traveled and returned back to the source. Now if you see the solution space for the 
    entire analytical solution think of adjacency matrix columns to be ordered from the start city to cover all cities in the travel in adjacent routes to again return to the start city. 
    If there are n cities in the graph and there are k cities to be covered one has to search the possibilities of 
    \( nP_k \) which is a permutation 
    \( n!/(n-k)! \) and for an analytical solution 
    to reach a fully closed form, one should limit to a maximum like the function be

    <div class="box">
        \( y = Lt_{n->inf,k->n}[n!/(n-k)!]  \)
    </div>
    <div class="box">
       \(  = Lt_{n->inf}[n!]  \)
    </div>
    There is no closed-form solution exists to this limit function due to it diverges to infinity and needs Sterling's approximation for precision. Also, it is surely a greater limit than ever 
    \( 2^n \) 
    <div class="box">
       \( Lt_{n->inf}[n!/2^n] -> inf \)
    </div>

    Surely diverges for 
    \( e^n \), which means the closed analytical solution is nearly impossible. Also, even the approximations are limited by the decimal precision of memory if you think
    the higher in n solutions. Also, Taylor's series till infinity is required to form a closed-form solution from a point space like the smart city. It makes one conclude TSP is not Polynomial for the reason of decimal precision requirement to infinity for fully closed form solution for at least e in memory. 
    Also, even though superposition in Qubits in Quantum computing allows for more states at once for a choice function again, its memory is limited by the no of qubits possible in real quantum 
    computers. So I would like to conclude P!=NP when the problem grows to infinity; therefore no closed-form solution. 
    </br>
    Thanks to the Euler and Bernoulli family for a rich wealth of mathematics at such an early age.
    </br>
    If we redefine closed-form solutions of any equation as those whose differential limit leads to zero. The differential limit is repeated differentiation until it leads to zero. For example

    <div class="box">
       \( Lt_{d()->0}[x^n] -> 0 \)
    </div>

    <div class="box">
       \( Lt_{d()->c}[x^n] -> n! \)
    </div>

    With this definition, let's see some examples of nonclosed forms that need numerical approximation solutions. First, let's see the \( e^x \) Euler exponential function. It never reduces in even
    repeated differentiation so it is numerical to calculate the precision of e by numerical methods. Let's see another example of circle equations like \( x^2 + y^s = r^2 \). Its differential limit 
    never reduces to zero. Also, know that introducing a differential limit means one should have a function with an independent variable, so at least two-dimensional space.

    When you differentiate \( x^n \) repeatedly n times, it gives n!; when you repeat one more time, it goes to zero due to n being a constant. Let's apply limit of differentiation to factorial of x.

    <div class="box">
       \( Lt_{d()->0}[x!] -> ? \)
    </div>
    

    Euler also had other ways to link numerical methods with the logarithmic constant e. Let me elucidate

    if you assume an exponential function with the following properties
    <div class="box">
       \( y = e^{x}, y^{'}=y \)
    </div>

    Where e is a constant to be found out. Let us see how this function behaves in Taylor's series

    Taylor's series is
    
    \( y = y(a) + y^{'}(a)(x-a) + y^{''}(a)(x-a)^2/2! + y^{'''}(a)(x-a)^3/3!.... \)

    <div class="box">
       \( y = E_{n=0}^{inf}[y^n(a)(x-a)^n/n!] \)
    </div>
    when \( y=y^n \) due to assumption \( y^{'}=y \)
    <div class="box">
       \( y = y(a)*E_{n=0}^{inf}[(x-a)^n/n!] \)
    </div>

    But y(0)=1,
    <div class="box">
       \( y = E_{n=0}^{inf}[x^n/n!] \)
    </div>
    <div class="box">
       \( e^x = E_{n=0}^{inf}[x^n/n!] \)
    </div>
    <div class="box">
       \( e = E_{n=0}^{inf}[1/n!] \)
    </div>
    If you sum even five terms 1+1+1/2+1/6+1/24+1/120 = 2.7166666... which is correct up to two decimal points
</body>
</html>

