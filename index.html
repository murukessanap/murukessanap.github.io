<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MathJax Example</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    Hello World

<h1>Today topic is Recommender Systems</h1>

Lets say a company like Netflix, Amazon, Disney Hotstar need to recommend the correct user profile the matching movie, so that there is a high chance that many users
will see the recommended movie thereby increasing profit
for the companies in many ways.
</br>
</br>
</br>
Lets see what are all the data available for the recommender system to learn from. There are user ratings movie, user profile information and movie information.
</br>
User rating movie is taken from the movie reviews given by users.
</br>
User profile information is all the information they ask you and you give them when you create an account and after which anything you do in the website, can also 
be tracked to have more info about the user, like watching a trailer, searching for a genre, even avoiding some content, etc.
</br>
Movie information is like the cast in the movie, genre, director, producing company, animated or not, country of origin, etc.
</br>
</br>
</br>
Lets assume all these information as matrices like user-movie rating as \(M_{um}\), user-profile as \(M_{up}\) and movie-profile as \(M_{mp}\)
</br>
    Now the target is to find missing ratings that are not given by the users for movies in \(M_{um}\)
    </br>
    Lets say the dimensions of the matrices \(M_{um}\) is \(_{uxm}\), \(M_{up}\) is \(_{f1xu}\) and \(M_{mp}\) is \(_{mxf2}\)
        </br>
        </br>
        Lets take weight matrices \(M_u\) and \(M_m\) that learns user features and movie features.
        </br>
        Dimensions of \(M_m\) and \(M_u\) are \(_{f2xu}\) and \(_{f1xm}\) respectively.
</br>
</br>
</br>
    The optimization objective is to learn \(M_u\) and \(M_m\) in the following setting.
    </br></br>
        \(M_{up}^T\) . \(M_u\) = \(M_{um}\)  and (1)
    </br>
        \(M_{mp}\) . \(M_m\) = \(M_{um}^T\)  and (2)
    </br></br>
There are various ways to solve this optimization objective.
</br>
One of them is through Gradient Descent Algorithm.
</br>
Randomized initial weights of the matrices \(M_u\) and \(M_m\) are updated via gradient descent by minimizing the following loss function.
</br></br>
    \(E(M_{um}^p - M_{um})^2\)  condition i-th element in \(M_{um}\) is not empty and where \(M_{um}^p\) is the predicted matrix and it is a sum of squarred errors of elementwise difference
    in the predicted and real user movie ratings matrix. 

</br>
</br>
</br>
    They call this colloborative filtering since there is a colloboration of all users' ratings, user profile information and movie information for
    getting the user the right recommendation. Also the practical implementation of gradient descent will be from solving (1) to (2) to (1) to (2) to (1) .....
    </br></br>
    Remember anything from social media, youtube, even ads on all of them are a form of recommendation.
</br>
    Have a wonderful recommendation !!!

    I owe this article in open forum to Andrew Ng, co-founder of Coursera. Thank you Andrew.
</br>
</br>
</br>

<h1>Today's topic is Information Theory</h1>
    The information theory was proposed by Shannon with his famous coining term entropy to quantify information as suggested by Von Neumann. But lets take a dive into what it could be 
    the use of an overall information theory for the human race. With such a core thought in mind, let me elucidate the information theory. Lets visit a different science actually physics by 
    Boltzman when he had to coin the entropy for physical systems such a group of atoms juggling in a container. The question was to find the initial set of possible states of the atoms and their
    relative positions and other properties may be like motion therefore heat and so on. So if you have two states for every atom as position and velocity. Then for different possible combinations of 
    the initial states of the atoms in different positions and velocities, it could end up in exponentially possible states in the flow of time. The exponentially increases over time and again.
    Lets consider an example, assume there are 10 different spacial points where atoms can possibly stay and 8 different directions in which they can have their velocities. Let there be some 7 different 
    possible velocities. This may seem absurd like the space, direction, speed in reality may seam in real number space. But in atomic quantum states it may be different. It could be quantized.

</br>
</br>
    So if you have an atom, it can have possible initial 10*8*7 states. Lets say there are two atoms then there are 10*8*7 and 10*8*7 possible initial states that start to interact with each other over time.
    So when there are more than one atom, you need to formulate some interacting principles between them and define the possibility of the states that may end up over time. The truth being if you have more
    atoms, with more freedom of space, direction and speed, whatever the interacting principles being the same for all atoms may end up in exponentially more and more possible states over time. With the caveat,
    the states may even repeat over time given the possible state of systems reaching a limit in the system, rules and the atoms. If you have to find out the intial state or at least the 
    repeating state of any n-body atomic system, given the possibilities of the states they are in over time, You need to do logarithm  of all the possible states. Let me elucidate with a simple 
    digital system. Lets say in binary system there are two possibilities for every digit 0 or 1. But if you have more than 1 digit, there is an exponentially increasing possible representative numbers or
    possible states. Like 1 digit has 2 states(0|1), 2 digits has 4 states(00|01|10|11), 3 digits has 8 states(000|001|010|011|100|101|110|111) and 4 digits has 16 states. So it is exponentially increasing
    over the increase of no of digits \(2^n\). Then think of the single bit having 10*8*7 possible states, then it could well possibly be \((10*8*7)^n\) states to end up in n amount of time.
    So to find the amount of information in so many possibilities one has to do logarithm, like if you have 8 numbers in a binary system then \(log_28=3\) there are 3 digital bits of information. If you have 
    \((10*8*7)^5\) states then \(log_{10*8*7}{10*8*7}^5=5\) resulting in 5 atoms being present in the inital state.

</br>
</br>
    The mathematical beauty of Shannon's theory lies in quantifying information over the probability of the loss of information. Lets say the inital state of atoms are not deterministic but
    probabilistic like there is a probability of 0.3 for say 1000 states, then 0.4 for say 2000 states and 0.3 for say 3000 states. Then the information in those possible states is 
    \( 0.3*1000 + 0.4*2000 + 0.3*3000 = 2000 \). But how can you get that information by seeing the system evolve over time over loss of information like in transmitting information over 
    any communication systems with a lot of probability of interference or error resulting in loss of information? So Shannon invented the formula 
    \( Entropy = \sum_i p_i * log (p_i) \)
    
</body>
</html>

